{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating an AI model curating the data to train the model is most important consideration. Using meaningful variables while taking in to account different facets of the game can create a powerful model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player-Specific Variables\n",
    "### General Player Information\n",
    "    - Player ID: Unique identifier for each player\n",
    "    - Position: Position played (QB, WR, RB, TE)\n",
    "    - Team: Player's team\n",
    "    - Height, Weight, Age: Physical attributes\n",
    "    \n",
    "### Historical Performance Metrics\n",
    "    - Games Played: Number of games played\n",
    "    - Games Started: Number of games started\n",
    "    - Snaps Played: Number of snaps played\n",
    "    - Passing Yards (QB): Total passing yards\n",
    "    - Passing Touchdowns (QB): Total passing touchdowns\n",
    "    - Interceptions (QB): Number of interceptions thrown\n",
    "    - Rushing Yards (RB/QB/WR/TE): Total rushing yards\n",
    "    - Rushing Touchdowns (RB/QB/WR/TE): Total rushing touchdowns\n",
    "    - Receptions (WR/TE/RB): Number of receptions\n",
    "    - Receiving Yards (WR/TE/RB): Total receiving yards\n",
    "    - Receiving Touchdowns (WR/TE/RB): Total receiving touchdowns\n",
    "    - Targets (WR/TE/RB): Number of targets\n",
    "    - Fumbles: Number of fumbles\n",
    "\n",
    "### Team-Specific Variables\n",
    "    - Team Offensive Metrics: Overall offensive performance of the team (total yards, points scored, etc.)\n",
    "    - Team Defensive Metrics: Overall defensive performance of the team (yards allowed, points allowed, etc.)\n",
    "    - Offensive Line Strength: Metrics like sacks allowed, pressures allowed, etc.\n",
    "    - Defensive Line Strength: Metrics like sacks made, pressures applied, etc.\n",
    "    - Injuries: Current injury report for the team\n",
    "\n",
    "### Opponent-Specific Variables\n",
    "    - Opponent Team Defensive Metrics: Performance metrics of the opposing team's defense (yards allowed, points allowed, turnovers forced, etc.)\n",
    "    - Opponent Defensive Schemes: Information about the defensive schemes commonly used by the opponent (e.g., 4-3 defense, zone coverage tendencies, etc.)\n",
    "    - Matchup History: Historical performance of the player/team against the upcoming opponent\n",
    "    - Opponent Injuries: Current injury report for the opposing team\n",
    "\n",
    "### Coaching and Strategy Variables\n",
    "    - Head Coach and Coordinators: Information about the head coach, offensive coordinator, and defensive coordinator\n",
    "    - Play Calling Tendencies: Historical data on play calling tendencies (run vs. pass ratio, play action usage, etc.)\n",
    "    - Scheme Preferences: Offensive and defensive scheme preferences (e.g., West Coast offense, Cover 2 defense)\n",
    "    - Game Plan Adjustments: Any reported adjustments or strategies for the upcoming game\n",
    "\n",
    "### Contextual and Situational Variables\n",
    "    - Game Location: Home or away game\n",
    "    - Weather Conditions: Weather forecast for the game day (temperature, precipitation, wind speed, etc.)\n",
    "    - Game Type: Regular season, playoff, or preseason game\n",
    "    - Time of Season: Early, mid, or late season\n",
    "    - Time of Game: Day game, night game, or prime-time game\n",
    "    - Field Surface: Type of field surface (grass, turf, etc.)\n",
    "    - Vegas Odds: Betting lines, point spreads, and over/under totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 15:35:57.897478: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-31 15:35:57.903844: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-31 15:35:57.919158: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 15:35:57.953496: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 15:35:57.965481: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 15:35:57.987112: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-31 15:35:59.393264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722461762.084624 1238821 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-31 15:36:02.087309: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/ncote/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    108\u001b[0m loss, mae, mse \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_features, test_target, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/array_slicing.py:492\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m    489\u001b[0m split_at \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(batch_dim \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m validation_split)))\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m batch_dim:\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples, which is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msufficient to split it into a validation and training set as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified by `validation_split=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide more data, or a different value for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_split` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split\u001b[39m(t, start, end):\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Ensure GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Example dataset\n",
    "player_stats_data = {\n",
    "    'Player ID': [1, 2],\n",
    "    'Name': ['Player A', 'Player B'],\n",
    "    'Team': ['XYZ', 'XYZ'],\n",
    "    'Position': ['QB', 'WR'],\n",
    "    'Games Played': [10, 10],\n",
    "    'Passing Yards': [2500, 0],\n",
    "    'Passing TDs': [20, 0],\n",
    "    'Interceptions': [5, 0],\n",
    "    'Rushing Yards': [200, 100],\n",
    "    'Receptions': [0, 50],\n",
    "    'Receiving Yards': [0, 600],\n",
    "    'Receiving TDs': [0, 6],\n",
    "    'Targets': [0, 75],\n",
    "    'Fumbles': [2, 1]\n",
    "}\n",
    "\n",
    "team_stats_data = {\n",
    "    'Team': ['XYZ'],\n",
    "    'Offensive Yards': [4000],\n",
    "    'Points Scored': [300],\n",
    "    'Defensive Yards Allowed': [3500],\n",
    "    'Points Allowed': [250],\n",
    "    'Sacks Allowed': [15],\n",
    "    'Sacks Made': [20],\n",
    "    'Current Injuries': ['Player A (Out), Player B (Q)']\n",
    "}\n",
    "\n",
    "opponent_stats_data = {\n",
    "    'Opponent': ['ABC'],\n",
    "    'Defensive Yards Allowed': [3200],\n",
    "    'Points Allowed': [200],\n",
    "    'Turnovers Forced': [15],\n",
    "    'Sacks Made': [18],\n",
    "    'Defensive Scheme': ['4-3, Zone Coverage'],\n",
    "    'Current Injuries': ['Player C (Out), Player D (Q)']\n",
    "}\n",
    "\n",
    "contextual_data = {\n",
    "    'Game ID': [101],\n",
    "    'Home Team': ['XYZ'],\n",
    "    'Away Team': ['ABC'],\n",
    "    'Location': ['Home'],\n",
    "    'Weather': ['Clear'],\n",
    "    'Game Type': ['Regular'],\n",
    "    'Time of Season': ['Mid'],\n",
    "    'Time of Game': ['Night'],\n",
    "    'Field Surface': ['Turf'],\n",
    "    'Vegas Odds': ['XYZ -3.5']\n",
    "}\n",
    "\n",
    "# Convert dictionaries to DataFrames\n",
    "player_stats_df = pd.DataFrame(player_stats_data)\n",
    "team_stats_df = pd.DataFrame(team_stats_data)\n",
    "opponent_stats_df = pd.DataFrame(opponent_stats_data)\n",
    "contextual_data_df = pd.DataFrame(contextual_data)\n",
    "\n",
    "# Merge data into a single DataFrame\n",
    "merged_data = pd.concat([player_stats_df, team_stats_df, opponent_stats_df, contextual_data_df], axis=1)\n",
    "\n",
    "# Select features and target\n",
    "features = merged_data[['Games Played', 'Passing Yards', 'Passing TDs', 'Interceptions', 'Rushing Yards', \n",
    "                        'Receptions', 'Receiving Yards', 'Receiving TDs', 'Targets', 'Fumbles',\n",
    "                        'Offensive Yards', 'Points Scored', 'Defensive Yards Allowed', 'Points Allowed',\n",
    "                        'Sacks Allowed', 'Sacks Made', 'Defensive Yards Allowed', 'Points Allowed', \n",
    "                        'Turnovers Forced', 'Sacks Made']]\n",
    "\n",
    "target = merged_data[['Passing Yards', 'Passing TDs', 'Rushing Yards', 'Receptions', 'Receiving Yards', 'Receiving TDs']]\n",
    "\n",
    "# Normalize the data\n",
    "features = (features - features.mean()) / features.std()\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "features = np.array(features)\n",
    "target = np.array(target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_features = features[:int(0.8 * len(features))]\n",
    "test_features = features[int(0.8 * len(features)):]\n",
    "\n",
    "train_target = target[:int(0.8 * len(target))]\n",
    "test_target = target[int(0.8 * len(target)):]\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[train_features.shape[1]]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(train_target.shape[1])\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_features, train_target, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae, mse = model.evaluate(test_features, test_target, verbose=2)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))\n",
    "\n",
    "# Predict future games\n",
    "predictions = model.predict(test_features)\n",
    "print(\"Predictions: \", predictions)\n",
    "\n",
    "# Continuously train with new data (dummy new data here for demonstration)\n",
    "new_features = test_features\n",
    "new_target = test_target\n",
    "\n",
    "# Re-train the model with new data\n",
    "model.fit(new_features, new_target, epochs=50, validation_split=0.2, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
